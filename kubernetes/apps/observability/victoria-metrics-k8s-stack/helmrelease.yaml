---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/helmrelease-helm-v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: victoria-metrics-k8s-stack
spec:
  interval: 30m
  timeout: 20m
  chartRef:
    kind: OCIRepository
    name: victoria-metrics-k8s-stack
  install:
    crds: CreateReplace
    remediation:
      retries: -1
  upgrade:
    cleanupOnFail: true
    crds: CreateReplace
    remediation:
      retries: 3
  values:
    # Disable Grafana since we're using separate deployment
    grafana:
      enabled: false

    # Use VMSingle for homelab deployment
    # Note: Default maxLabelsPerTimeseries (30) is sufficient with proper label filtering
    # High cardinality addressed by dropping NFD labels in relabelConfigs (not metricRelabelConfigs)
    # This ensures the synthetic "up" metric also has filtered labels, preventing rejection
    #
    # Memory sizing rationale (2025-10-17):
    # - Increased from 2Gi to 4Gi to resolve query timeout issues
    # - 2Gi limit caused 86% memory usage, triggering "timeout exceeded while fetching data block" errors
    # - VictoriaMetrics official docs recommend 4-10Gi for production workloads
    # - Default -memory.allowedPercent is 60% of container limit for internal caches
    # - Next steps if issues persist: tune query limits via extraArgs (-search.maxQueryDuration, -search.maxMemoryPerQuery)
    vmsingle:
      enabled: true
      spec:
        retentionPeriod: "14d"
        replicaCount: 1
        storage:
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: 30Gi
          storageClassName: ceph-block
        resources:
          limits:
            memory: 4Gi
          requests:
            cpu: 100m
            memory: 512Mi
        readinessProbe:
          httpGet:
            path: /health
            port: 8428
          timeoutSeconds: 30
          periodSeconds: 15
          failureThreshold: 5
          initialDelaySeconds: 30
        # TEMPORARY: Avoid nami node until NVMe OSD migration completes
        # Remove this affinity rule after Samsung 990 Pro NVMe is installed and BX500 SATA OSD is removed
        # See: docs/memory-bank/ceph-osd-nvme-migration-nami.md
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: kubernetes.io/hostname
                  operator: NotIn
                  values: [nami]

    # Disable VMCluster for single-node deployment
    vmcluster:
      enabled: false

    # AlertManager configuration
    alertmanager:
      enabled: true
      spec:
        disableNamespaceMatcher: true
        replicaCount: 1
        storage:
          volumeClaimTemplate:
            spec:
              storageClassName: ceph-block
              accessModes:
              - ReadWriteOnce
              resources:
                requests:
                  storage: 1Gi
        resources:
          limits:
            memory: 128Mi
          requests:
            cpu: 10m
            memory: 32Mi
        configReloaderResources:
          limits:
            memory: 64Mi
          requests:
            cpu: 10m
            memory: 25Mi

    # VMAlert configuration
    vmalert:
      enabled: true
      spec:
        evaluationInterval: 15s
        selectAllByDefault: true
        datasource:
          url: http://vmsingle-victoria-metrics-k8s-stack.observability:8428
        remoteWrite:
          url: http://vmsingle-victoria-metrics-k8s-stack.observability:8428
        remoteRead:
          url: http://vmsingle-victoria-metrics-k8s-stack.observability:8428
        resources:
          limits:
            memory: 128Mi
          requests:
            cpu: 10m
            memory: 32Mi
        configReloaderResources:
          limits:
            memory: 32Mi
          requests:
            cpu: 10m
            memory: 25Mi
        # Override chart defaults to enable automatic API path appending
        # Without this, VMAlert sends requests to "/" instead of "/api/v1/write"
        # Reference: https://docs.victoriametrics.com/vmalert/#remote-write
        extraArgs:
          remoteWrite.disablePathAppend: "false"

    # VMAgent configuration
    vmagent:
      enabled: true
      spec:
        scrapeInterval: 20s
        selectAllByDefault: true
        resources:
          limits:
            memory: 512Mi
          requests:
            cpu: 50m
            memory: 128Mi
        configReloaderResources:
          limits:
            memory: 64Mi
          requests:
            cpu: 10m
            memory: 25Mi
        extraArgs:
          promscrape.streamParse: "true"
          promscrape.dropOriginalLabels: "true"

    # Node exporter
    prometheus-node-exporter:
      enabled: true
      resources:
        limits:
          memory: 64Mi
        requests:
          cpu: 10m
          memory: 16Mi

    # Kube-state-metrics
    kube-state-metrics:
      enabled: true
      metricLabelsAllowlist:
      - pods=[*]
      - deployments=[*]
      - persistentvolumeclaims=[*]
      resources:
        limits:
          memory: 256Mi
        requests:
          cpu: 10m
          memory: 64Mi

    # Default rules and dashboards
    defaultRules:
      create: true
      additionalGroupByLabels: []
      # Individual rule overrides via defaultRules.rules.<RuleName>.create: false
      # This disables specific default rules while keeping the rule group enabled
      # VictoriaMetrics operator aggregates ALL VMRule resources - there is no true override mechanism
      # To customize a default rule: disable it here, then create custom VMRule with desired configuration
      # Reference: https://github.com/VictoriaMetrics/helm-charts chart values.yaml structure
      rules:
        TooManyLogs:
          create: false # Using custom VMRule (vmrule.yaml) with >20 threshold vs default >0
      groups:
        etcd:
          create: true
        # general recording rules disabled due to count:up0 producing no data when no targets are down
        # Recording rule: count(up == 0) without(instance,pod,node)
        # Returns empty result when all targets healthy, triggering RecordingRulesNoData alert
        # Same pattern as kubeScheduler - rule designed for dashboards, not critical for operations
        general:
          create: false
        # kubeScheduler recording rules disabled due to deprecated/removed metrics in Kubernetes 1.27+
        # Summary: Old scheduler histogram metrics don't exist, causing RecordingRulesNoData alerts
        # The new scheduler_scheduling_attempt_duration_seconds metric exists and is being scraped
        # See commit message (2025-10-11) for detailed investigation and upstream context
        kubeScheduler:
          create: false
        k8sContainerCpuLimits:
          create: true
        k8sContainerMemoryLimits:
          create: true
        k8sContainerResource:
          create: true
        kubeApiserver:
          create: true
        kubelet:
          create: true
        kubernetesApps:
          create: true
        kubernetesResources:
          create: true
        kubernetesStorage:
          create: true
        kubernetesSystem:
          create: true
        node:
          create: true
        vmagent:
          create: true
        vmsingle:
          create: true
        vmHealth:
          create: true

    # Kubernetes component monitoring
    kubeApiServer:
      enabled: true

    kubeControllerManager:
      enabled: true
      endpoints: []
      service:
        enabled: true
        port: 10257
        targetPort: 10257
        selector:
          component: kube-controller-manager
      vmScrape:
        spec:
          jobLabel: jobLabel
          namespaceSelector:
            matchNames:
            - kube-system
          endpoints:
          - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
            port: http-metrics
            scheme: https
            tlsConfig:
              insecureSkipVerify: true
              caFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt

    kubeScheduler:
      enabled: true
      endpoints: []
      service:
        enabled: true
        port: 10259
        targetPort: 10259
        selector:
          component: kube-scheduler
      vmScrape:
        spec:
          jobLabel: jobLabel
          namespaceSelector:
            matchNames:
            - kube-system
          endpoints:
          - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
            port: http-metrics
            scheme: https
            tlsConfig:
              insecureSkipVerify: true
              caFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt

    kubeEtcd:
      enabled: true
      endpoints: []
      service:
        enabled: true
        port: 2381
        targetPort: 2381
        selector:
          component: kube-apiserver
      vmScrape:
        enabled: true
        spec:
          jobLabel: jobLabel
          namespaceSelector:
            matchNames:
            - kube-system
          endpoints:
          - port: http-metrics
            scheme: http

    coreDns:
      enabled: true
      service:
        enabled: true
        port: 9153
        targetPort: 9153
        selector:
          k8s-app: kube-dns

    kubeProxy:
      enabled: false

    # Kubelet monitoring
    kubelet:
      enabled: true
      vmScrapes:
        cadvisor:
          enabled: true
          spec:
            path: /metrics/cadvisor
        probes:
          enabled: true
          spec:
            path: /metrics/probes
        resources:
          enabled: true
          spec:
            path: /metrics/resource
        kubelet:
          spec: {}
      vmScrape:
        kind: VMNodeScrape
        spec:
          scheme: "https"
          honorLabels: true
          interval: "30s"
          scrapeTimeout: "5s"
          tlsConfig:
            insecureSkipVerify: true
            caFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
          # CRITICAL: Label filtering must happen in relabelConfigs, not metricRelabelConfigs
          # relabelConfigs applies to ALL metrics including synthetic "up" metric
          # metricRelabelConfigs only applies to scraped metrics, causing "up" to exceed label limits
          # Pattern: labelmap → labeldrop (immediately filter) → add operational labels
          relabelConfigs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)
          - action: labeldrop
            regex: (uid)
          - action: labeldrop
            regex: (id|name)
          - action: labeldrop
            regex: (beta_kubernetes_io_.*)
          - action: labeldrop
            regex: (feature_node_kubernetes_io_.*)
          - action: labeldrop
            regex: (extensions_talos_dev_.*)
          - sourceLabels: [__metrics_path__]
            targetLabel: metrics_path
          - targetLabel: job
            replacement: kubelet
          # metricRelabelConfigs for dropping entire metrics (not labels)
          metricRelabelConfigs:
          - action: drop
            source_labels: [__name__]
            regex: (rest_client_request_duration_seconds_bucket|rest_client_request_duration_seconds_sum|rest_client_request_duration_seconds_count)

    # VictoriaMetrics Operator
    victoria-metrics-operator:
      enabled: true
      operator:
        disable_prometheus_converter: false
      serviceMonitor:
        enabled: true
      resources:
        limits:
          memory: 256Mi
        requests:
          cpu: 50m
          memory: 64Mi
