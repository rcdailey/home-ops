---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/helmrelease-helm-v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: victoria-metrics-k8s-stack
spec:
  interval: 30m
  timeout: 20m
  chartRef:
    kind: OCIRepository
    name: victoria-metrics-k8s-stack
  # WORKAROUND: Chart creates VMRule even when defaultRules.groups.general.create: false
  # Patch the count:up0 recording rule to always return a value (prevents RecordingRulesNoData alerts)
  #
  # WHY this is needed:
  # - Recording rule count:up0 expression: count(up == 0) without(instance,pod,node)
  # - Returns empty result when all targets healthy (no targets with up == 0)
  # - Empty results are EXPECTED BEHAVIOR but trigger RecordingRulesNoData alerts
  # - Chart doesn't respect general.create: false and doesn't expose rule expression customization
  #
  # HOW it works:
  # - Adds 'OR on() vector(0)' to expression, ensuring it returns 0 when all targets healthy
  # - Uses JSON6902 patch to modify the VMRule spec after Helm renders it
  # - Path /spec/groups/0/rules/1/expr targets count:up0 (second rule in first group)
  #
  # WHEN to reconsider:
  # - Chart fixes create: false configuration or exposes rule expression customization
  # - Recording rules become operationally critical (currently optional dashboard optimizations)
  postRenderers:
  - kustomize:
      patches:
      - target:
          kind: VMRule
          name: victoria-metrics-k8s-stack-kube-prometheus-general.rules
        patch: |-
          - op: replace
            path: /spec/groups/0/rules/1/expr
            value: "count(up == 0) without(instance,pod,node) OR on() vector(0)"
  install:
    crds: CreateReplace
    remediation:
      retries: -1
  upgrade:
    cleanupOnFail: true
    crds: CreateReplace
    remediation:
      retries: 3
  values:
    # Disable Grafana since we're using separate deployment
    grafana:
      enabled: false

    # Use VMSingle for homelab deployment
    # Note: Default maxLabelsPerTimeseries (30) is sufficient with proper label filtering
    # High cardinality addressed by dropping NFD labels in relabelConfigs (not metricRelabelConfigs)
    # This ensures the synthetic "up" metric also has filtered labels, preventing rejection
    #
    # Memory sizing rationale (2025-10-17):
    # - Increased from 2Gi to 4Gi to resolve query timeout issues
    # - 2Gi limit caused 86% memory usage, triggering "timeout exceeded while fetching data block" errors
    # - VictoriaMetrics official docs recommend 4-10Gi for production workloads
    # - Default -memory.allowedPercent is 60% of container limit for internal caches
    # - Next steps if issues persist: tune query limits via extraArgs (-search.maxQueryDuration, -search.maxMemoryPerQuery)
    vmsingle:
      enabled: true
      spec:
        retentionPeriod: "14d"
        replicaCount: 1
        storage:
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: 30Gi
          # OpenEBS hostPath provisioner for ephemeral metrics storage
          # WHY: Avoid Ceph write amplification from frequent metric writes
          storageClassName: openebs-hostpath
        # Pin to lucy node - NVMe storage required for sustained write performance
        # Nami's Crucial BX500 SATA SSD has severe latency spikes (6-38s at p99.99) causing deadlocks
        # Lucy's Crucial P3 NVMe has DRAM cache and significantly better sustained performance
        nodeSelector:
          kubernetes.io/hostname: lucy
        resources:
          limits:
            memory: 4Gi
          requests:
            cpu: 100m
            memory: 512Mi
        livenessProbe: &probes
          httpGet:
            path: /health
            port: 8428
        readinessProbe: *probes

    # Disable VMCluster for single-node deployment
    vmcluster:
      enabled: false

    # AlertManager configuration
    alertmanager:
      enabled: true
      spec:
        disableNamespaceMatcher: true
        externalURL: https://vmalertmanager.${SECRET_DOMAIN}
        replicaCount: 1
        storage:
          volumeClaimTemplate:
            spec:
              # OpenEBS hostPath provisioner for ephemeral alert state storage
              # WHY: Avoid Ceph write amplification from frequent alert state writes
              storageClassName: openebs-hostpath
              accessModes:
              - ReadWriteOnce
              resources:
                requests:
                  storage: 1Gi
        # Pin to non-controlplane nodes only (avoid controlplane etcd contention)
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: node-role.kubernetes.io/control-plane
                  operator: DoesNotExist
        resources:
          limits:
            memory: 128Mi
          requests:
            cpu: 10m
            memory: 32Mi
        configReloaderResources:
          limits:
            memory: 64Mi
          requests:
            cpu: 10m
            memory: 25Mi

    # VMAlert configuration
    vmalert:
      enabled: true
      spec:
        evaluationInterval: 15s
        selectAllByDefault: true
        datasource:
          url: http://vmsingle-victoria-metrics-k8s-stack.observability:8428
        remoteWrite:
          url: http://vmsingle-victoria-metrics-k8s-stack.observability:8428
        remoteRead:
          url: http://vmsingle-victoria-metrics-k8s-stack.observability:8428
        resources:
          limits:
            memory: 128Mi
          requests:
            cpu: 10m
            memory: 32Mi
        configReloaderResources:
          limits:
            memory: 64Mi
          requests:
            cpu: 10m
            memory: 25Mi
        extraArgs:
          # Override chart defaults to enable automatic API path appending
          # Without this, VMAlert sends requests to "/" instead of "/api/v1/write"
          # Reference: https://docs.victoriametrics.com/vmalert/#remote-write
          remoteWrite.disablePathAppend: "false"
          external.url: https://vmalert.${SECRET_DOMAIN}

    # VMAgent configuration
    vmagent:
      enabled: true
      spec:
        scrapeInterval: 20s
        # Opt-in metrics: Scrape infrastructure by default, application metrics require explicit labels
        # Infrastructure (node-exporter, kube-state-metrics, kubelet) always scraped via chart VMNodeScrape objects
        # Application metrics only from specific namespaces (media, default, home)
        selectAllByDefault: false
        # Scrape VMServiceScrape/VMPodScrape only from target namespaces
        serviceScrapeNamespaceSelector:
          matchExpressions:
          - key: kubernetes.io/metadata.name
            operator: In
            values: [media, default, home, observability]
        podScrapeNamespaceSelector:
          matchExpressions:
          - key: kubernetes.io/metadata.name
            operator: In
            values: [media, default, home, observability]
        # Node scrapes always enabled (infrastructure metrics)
        nodeScrapeSelector: {}
        nodeScrapeNamespaceSelector: {}
        # Enable chart-created scrape configs (infrastructure components)
        serviceScrapeSelector: {}
        podScrapeSelector: {}
        probeSelector: {}
        probeNamespaceSelector:
          matchExpressions:
          - key: kubernetes.io/metadata.name
            operator: In
            values: [observability]
        staticScrapeSelector: {}
        resources:
          limits:
            memory: 512Mi
          requests:
            cpu: 50m
            memory: 128Mi
        configReloaderResources:
          limits:
            memory: 64Mi
          requests:
            cpu: 10m
            memory: 25Mi
        extraArgs:
          promscrape.streamParse: "true"
          promscrape.dropOriginalLabels: "true"

    # Node exporter
    prometheus-node-exporter:
      enabled: true
      resources:
        limits:
          memory: 64Mi
        requests:
          cpu: 10m
          memory: 16Mi

    # Kube-state-metrics
    kube-state-metrics:
      enabled: true
      metricLabelsAllowlist:
      - pods=[*]
      - deployments=[*]
      - persistentvolumeclaims=[*]
      resources:
        limits:
          memory: 512Mi
        requests:
          cpu: 10m
          memory: 128Mi
      rbac:
        extraRules:
        - apiGroups:
          - kustomize.toolkit.fluxcd.io
          - helm.toolkit.fluxcd.io
          - source.toolkit.fluxcd.io
          resources:
          - kustomizations
          - helmreleases
          - gitrepositories
          - helmrepositories
          - ocirepositories
          verbs: [list, watch]
      customResourceState:
        enabled: true
        config:
          spec:
            resources:
            - groupVersionKind:
                group: kustomize.toolkit.fluxcd.io
                version: v1
                kind: Kustomization
              metricNamePrefix: gotk
              metrics:
              - name: resource_info
                help: The current state of a Flux Kustomization resource
                each:
                  type: Info
                  info:
                    labelsFromPath:
                      name: [metadata, name]
                labelsFromPath:
                  exported_namespace: [metadata, namespace]
                  ready: [status, conditions, "[type=Ready]", status]
                  suspended: [spec, suspend]
                  revision: [status, lastAppliedRevision]
                  source_name: [spec, sourceRef, name]
            - groupVersionKind:
                group: helm.toolkit.fluxcd.io
                version: v2
                kind: HelmRelease
              metricNamePrefix: gotk
              metrics:
              - name: resource_info
                help: The current state of a Flux HelmRelease resource
                each:
                  type: Info
                  info:
                    labelsFromPath:
                      name: [metadata, name]
                labelsFromPath:
                  exported_namespace: [metadata, namespace]
                  ready: [status, conditions, "[type=Ready]", status]
                  suspended: [spec, suspend]
                  revision: [status, history, "0", chartVersion]
                  chart_name: [status, history, "0", chartName]

    # Default rules and dashboards
    defaultRules:
      create: true
      additionalGroupByLabels: []
      # Individual rule overrides via defaultRules.rules.<RuleName>.create: false
      # This disables specific default rules while keeping the rule group enabled
      # VictoriaMetrics operator aggregates ALL VMRule resources - there is no true override mechanism
      # To customize a default rule: disable it here, then create custom VMRule with desired configuration
      # Reference: https://github.com/VictoriaMetrics/helm-charts chart values.yaml structure
      rules:
        TooManyLogs:
          create: false # Using custom VMRule (vmrule.yaml) with >20 threshold vs default >0
        NodeSystemSaturation:
          create: false # Using custom VMRule (vmrule.yaml) requiring both high load AND high CPU
        KubeHpaMaxedOut:
          create: false # Using custom VMRule (vmrule.yaml) excluding KEDA HPAs
      groups:
        etcd:
          create: false
        # general recording rules patched via postRenderers (see above) to prevent RecordingRulesNoData alerts
        # Chart creates VMRule even when create: false, so we patch the expression instead
        general:
          create: true
        # kubeScheduler recording rules disabled due to deprecated/removed metrics in Kubernetes 1.27+
        # Summary: Old scheduler histogram metrics don't exist, causing RecordingRulesNoData alerts
        # The new scheduler_scheduling_attempt_duration_seconds metric exists and is being scraped
        # See commit message (2025-10-11) for detailed investigation and upstream context
        kubeScheduler:
          create: false
        k8sContainerCpuLimits:
          create: true
        k8sContainerMemoryLimits:
          create: true
        k8sContainerResource:
          create: true
        kubeApiserver:
          create: true
        kubelet:
          create: true
        kubernetesApps:
          create: true
        kubernetesResources:
          create: true
        kubernetesStorage:
          create: true
        kubernetesSystem:
          create: true
        node:
          create: true
        vmagent:
          create: true
        vmsingle:
          create: true
        vmHealth:
          create: true

    # Kubernetes component monitoring
    kubeApiServer:
      enabled: true

    kubeControllerManager:
      enabled: true
      endpoints: []
      service:
        enabled: true
        port: 10257
        targetPort: 10257
        selector:
          component: kube-controller-manager
      vmScrape:
        spec:
          jobLabel: jobLabel
          namespaceSelector:
            matchNames:
            - kube-system
          endpoints:
          - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
            port: http-metrics
            scheme: https
            tlsConfig:
              insecureSkipVerify: true
              caFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt

    kubeScheduler:
      enabled: true
      endpoints: []
      service:
        enabled: true
        port: 10259
        targetPort: 10259
        selector:
          component: kube-scheduler
      vmScrape:
        spec:
          jobLabel: jobLabel
          namespaceSelector:
            matchNames:
            - kube-system
          endpoints:
          - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
            port: http-metrics
            scheme: https
            tlsConfig:
              insecureSkipVerify: true
              caFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt

    kubeEtcd:
      enabled: false

    coreDns:
      enabled: true
      service:
        enabled: true
        port: 9153
        targetPort: 9153
        selector:
          k8s-app: kube-dns

    kubeProxy:
      enabled: false

    # Kubelet monitoring
    kubelet:
      enabled: true
      vmScrapes:
        cadvisor:
          enabled: true
          spec:
            path: /metrics/cadvisor
        probes:
          enabled: true
          spec:
            path: /metrics/probes
        resources:
          enabled: true
          spec:
            path: /metrics/resource
        kubelet:
          spec: {}
      vmScrape:
        kind: VMNodeScrape
        spec:
          scheme: "https"
          honorLabels: true
          interval: "30s"
          scrapeTimeout: "5s"
          tlsConfig:
            insecureSkipVerify: true
            caFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
          # CRITICAL: Label filtering must happen in relabelConfigs, not metricRelabelConfigs
          # relabelConfigs applies to ALL metrics including synthetic "up" metric
          # metricRelabelConfigs only applies to scraped metrics, causing "up" to exceed label limits
          # Pattern: labelmap → labeldrop (immediately filter) → add operational labels
          relabelConfigs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)
          - action: labeldrop
            regex: (uid)
          - action: labeldrop
            regex: (id|name)
          - action: labeldrop
            regex: (beta_kubernetes_io_.*)
          - action: labeldrop
            regex: (feature_node_kubernetes_io_.*)
          - action: labeldrop
            regex: (extensions_talos_dev_.*)
          - sourceLabels: [__metrics_path__]
            targetLabel: metrics_path
          - targetLabel: job
            replacement: kubelet
          # metricRelabelConfigs for dropping entire metrics (not labels)
          metricRelabelConfigs:
          - action: drop
            source_labels: [__name__]
            regex: (rest_client_request_duration_seconds_bucket|rest_client_request_duration_seconds_sum|rest_client_request_duration_seconds_count)

    # VictoriaMetrics Operator
    victoria-metrics-operator:
      enabled: true
      operator:
        disable_prometheus_converter: false
      serviceMonitor:
        enabled: true
      resources:
        limits:
          memory: 256Mi
        requests:
          cpu: 50m
          memory: 64Mi
