---
# yaml-language-server: $schema=https://kubernetes-schemas.pages.dev/operator.victoriametrics.com/vmrule_v1beta1.json
apiVersion: operator.victoriametrics.com/v1beta1
kind: VMRule
metadata:
  name: flux-alerts
spec:
  groups:
  - name: flux-health
    interval: 1m
    rules:
    # Alert on Kustomizations not ready for >30m
    # WHY: Detects all Kustomization failures including dependency issues and reconciliation errors
    # WHAT: Fires when Kustomization ready status is False/Unknown for >30m
    # HOW: Uses gotk_resource_info metric from kube-state-metrics with ready label
    - alert: FluxKustomizationNotReady
      expr: |
        gotk_resource_info{customresource_kind="Kustomization",ready=~"False|Unknown"} == 1
      for: 30m
      labels:
        severity: warning
      annotations:
        summary: "Flux Kustomization {{ $labels.name }} not ready for >30m"
        description: |
          Kustomization {{ $labels.name }} in namespace {{ $labels.exported_namespace }} has been unhealthy for over 30 minutes.
          Current status: ready={{ $labels.ready }}
          Check: kubectl describe kustomization {{ $labels.name }} -n {{ $labels.exported_namespace }}

    # Alert on Kustomizations stuck for >24h
    # WHY: Detects long-running failures like cascading dependency issues (NFD->intel-gpu-plugin->immich)
    # WHAT: Fires when Kustomization has been not ready for over 24 hours
    # HOW: Uses same metric with longer duration threshold
    - alert: FluxKustomizationStuck
      expr: |
        gotk_resource_info{customresource_kind="Kustomization",ready=~"False|Unknown"} == 1
      for: 24h
      labels:
        severity: critical
      annotations:
        summary: "Flux Kustomization {{ $labels.name }} stuck for >24h"
        description: |
          Kustomization {{ $labels.name }} in namespace {{ $labels.exported_namespace }} has been unhealthy for over 24 hours.
          This indicates a persistent failure requiring immediate investigation.
          Source: {{ $labels.source_name }}
          Check: kubectl describe kustomization {{ $labels.name }} -n {{ $labels.exported_namespace }}

    # Alert on HelmRelease not ready for >15m
    # WHY: Early detection of HelmRelease failures before they become critical
    # WHAT: Fires when HelmRelease ready status is False/Unknown for >15m
    # HOW: Uses gotk_resource_info metric from kube-state-metrics
    - alert: FluxHelmReleaseNotReady
      expr: |
        gotk_resource_info{customresource_kind="HelmRelease",ready=~"False|Unknown"} == 1
      for: 15m
      labels:
        severity: warning
      annotations:
        summary: "Flux HelmRelease {{ $labels.name }} not ready"
        description: |
          HelmRelease {{ $labels.name }} in namespace {{ $labels.exported_namespace }} has been unhealthy for over 15 minutes.
          Chart: {{ $labels.chart_name }}
          Check: kubectl describe helmrelease {{ $labels.name }} -n {{ $labels.exported_namespace }}

    # Alert on HelmRelease stalled/failed for >1h
    # WHY: Detects permanent failures requiring manual intervention (retries exhausted)
    # WHAT: Fires when HelmRelease remains unhealthy for over 1 hour
    # HOW: Longer duration threshold indicates stalled state needing intervention
    - alert: FluxHelmReleaseStalled
      expr: |
        gotk_resource_info{customresource_kind="HelmRelease",ready=~"False|Unknown"} == 1
      for: 1h
      labels:
        severity: critical
      annotations:
        summary: "Flux HelmRelease {{ $labels.name }} stalled/failed"
        description: |
          HelmRelease {{ $labels.name }} in namespace {{ $labels.exported_namespace }} has been unhealthy for over 1 hour.
          Manual intervention likely required - release may have exhausted retries.
          Chart: {{ $labels.chart_name }} ({{ $labels.revision }})
          Check: kubectl describe helmrelease {{ $labels.name }} -n {{ $labels.exported_namespace }}
---
# yaml-language-server: $schema=https://kubernetes-schemas.pages.dev/operator.victoriametrics.com/vmrule_v1beta1.json
apiVersion: operator.victoriametrics.com/v1beta1
kind: VMRule
metadata:
  name: vm-health-overrides
spec:
  groups:
  - name: vm-health
    interval: 30s
    concurrency: 2
    rules:
    # Override default TooManyLogs alert to ignore transient startup errors
    # The config-reloader sidecar logs harmless connection errors during pod startup
    # when it tries to connect to vmagent before the main container is ready.
    # This adjusted threshold only fires when errors persist beyond startup (>20 errors/5min).
    - alert: TooManyLogs
      expr: sum(increase(vm_log_messages_total{level="error"}[5m])) without (app_version, location) > 20
      for: 15m
      labels:
        severity: warning
      annotations:
        summary: Too many logs printed for job {{ $labels.job }} ({{ $labels.instance }})
        description: |
          Logging rate for job "{{ $labels.job }}" ({{ $labels.instance }}) is {{ $value }} for last 15m.
          Worth to check logs for specific error messages.

    # Override NodeSystemSaturation to exclude VM nodes
    # VMs report inflated load averages due to hypervisor scheduling, which doesn't
    # accurately reflect system saturation. The node_uname_info metric joined with
    # node feature discovery labels would be ideal, but we can filter by checking
    # actual CPU usage vs load to detect legitimate saturation.
    # Only alert when load is high AND actual CPU usage confirms saturation (>80%).
    - alert: NodeSystemSaturation
      expr: |
        (
          (node_load1{job="node-exporter"} / count(node_cpu_seconds_total{job="node-exporter",mode="idle"}) without(cpu,mode)) > 2
          and
          (1 - avg(rate(node_cpu_seconds_total{job="node-exporter",mode="idle"}[5m])) without(cpu)) > 0.8
        )
      for: 15m
      labels:
        severity: warning
      annotations:
        summary: System saturated, load per core is very high.
        description: |
          System load per core at {{ $labels.instance }} has been above 2 for the last 15 minutes, is currently at {{ printf "%.2f" $value }}.
          CPU usage is also high, confirming genuine saturation (not VM scheduling artifacts).
          This might indicate this instance resources saturation and can cause it becoming unresponsive.

    # Override Watchdog alert to add 'hidden' label for UI filtering
    # WHY: Watchdog is a dead man's switch for monitoring stack health
    # WHAT: Hide from VMAlertmanager/Grafana UIs while keeping functional
    # HOW: Add 'hidden: "true"' label for UI filtering
    - alert: Watchdog
      expr: vector(1)
      labels:
        severity: none
        hidden: "true"
      annotations:
        summary: Alerting pipeline health check (always firing)
        description: Dead man's switch for monitoring stack health verification.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/general/watchdog

    # Override KubeHpaMaxedOut to exclude KEDA-managed HPAs
    # WHY: KEDA HPAs scale 0->1 when active; running at max=1 is expected normal behavior
    # WHAT: Exclude HPAs with "keda-hpa-" prefix from the maxed-out alert
    # HOW: Filter out horizontalpodautoscaler names starting with "keda-hpa-"
    - alert: KubeHpaMaxedOut
      expr: |
        kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics",horizontalpodautoscaler!~"keda-hpa-.*"}
          == kube_horizontalpodautoscaler_spec_max_replicas{job="kube-state-metrics",horizontalpodautoscaler!~"keda-hpa-.*"}
      for: 15m
      labels:
        severity: warning
      annotations:
        summary: HPA is running at max replicas
        description: |
          HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler }} has been running at max replicas for longer than 15 minutes.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubehpamaxedout

    # Override TooHighChurnRate24h with relaxed threshold for homelab
    # WHY: Default 3x threshold is too aggressive for dynamic Kubernetes environments
    # WHAT: Normal pod churn (restarts, rolling updates) creates new series with different pod labels
    # HOW: Increase threshold from 3x to 10x active series to reduce false positives
    # WHEN to reconsider: If memory pressure or slow queries occur due to high cardinality
    - alert: TooHighChurnRate24h
      expr: |
        sum(increase(vm_new_timeseries_created_total[24h])) by(job)
          > (sum(vm_cache_entries{type="storage/hour_metric_ids"}) by(job) * 10)
      for: 15m
      labels:
        severity: warning
      annotations:
        summary: High time series churn rate for {{ $labels.job }}
        description: |
          New time series created over 24h exceeds 10x active series count.
          Current ratio: {{ $value | printf "%.1f" }}x
          This may indicate label cardinality issues or excessive pod restarts.

    # Threshold-based disk space alerts replacing predict_linear CephNodeDiskspaceWarning
    # WHY: predict_linear generates false positives during image pulls and temporary spikes
    # WHAT: Alert when disk usage exceeds fixed thresholds (80% warning, 90% critical)
    # HOW: Simple percentage calculation on node_filesystem metrics
    - alert: NodeDiskSpaceLow
      expr: |
        (
          (node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!=""} - node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!=""})
          / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!=""} * 100 > 80
        ) and (node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!=""} == 0)
      for: 15m
      labels:
        severity: warning
      annotations:
        summary: "Disk usage above 80% on {{ $labels.instance }}"
        description: |
          Filesystem {{ $labels.mountpoint }} on {{ $labels.instance }} is {{ printf "%.1f" $value }}% full.
          Device: {{ $labels.device }}

    - alert: NodeDiskSpaceCritical
      expr: |
        (
          (node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!=""} - node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!=""})
          / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!=""} * 100 > 90
        ) and (node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!=""} == 0)
      for: 10m
      labels:
        severity: critical
      annotations:
        summary: "Disk usage above 90% on {{ $labels.instance }}"
        description: |
          Filesystem {{ $labels.mountpoint }} on {{ $labels.instance }} is {{ printf "%.1f" $value }}% full.
          Immediate action required. Device: {{ $labels.device }}
