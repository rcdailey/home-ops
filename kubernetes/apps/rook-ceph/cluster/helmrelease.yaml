---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/helmrelease-helm-v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: rook-ceph-cluster
spec:
  interval: 1h
  chartRef:
    kind: OCIRepository
    name: rook-ceph-cluster
    namespace: rook-ceph
  dependsOn:
  - name: rook-ceph-operator
    namespace: rook-ceph
  install:
    remediation:
      retries: -1
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  values:
    # Rook Ceph Cluster Configuration
    # 3-node cluster with device selectors for each node
    monitoring:
      enabled: true
      createPrometheusRules: true
      prometheusRuleOverrides:
        # CRITICAL: Patch CephNodeDiskspaceWarning to exclude NFS/tmpfs from disk space alerts
        # Without this, NFS volumes (e.g., 160TB media share) trigger false warnings
        # See: https://github.com/rook/rook/issues/9082, https://github.com/rook/rook/pull/9837
        CephNodeDiskspaceWarning:
          expr: >-
            predict_linear(node_filesystem_free_bytes{device=~"/.*",fstype!~"nfs.*|tmpfs"}[2d], 3600 * 24 * 5)
            *on(instance) group_left(nodename) node_uname_info < 0
        # CRITICAL: Disable CephNodeNetworkPacketDrops alert for Intel NIC false positives
        # Intel NUCs (nami, marin) with eno1 interfaces show benign receive packet drops
        # due to hardware checksum offloading dropping invalid packets (multicast/broadcast noise)
        # Ceph cluster remains HEALTH_OK, indicating no operational impact
        # See: Intel I219/I225 NICs commonly drop packets with checksum offload enabled
        CephNodeNetworkPacketDrops:
          disabled: true

    ingress:
      dashboard:
        enabled: false

    toolbox:
      enabled: true

    cephClusterSpec:
      # Enable per-RBD image metrics for PVC monitoring dashboards
      # Required for ceph_rbd_* metrics (read/write IOPS, throughput, latency)
      # to populate Grafana dashboard panels. Without this, only cluster-level
      # metrics are available and per-PVC RBD performance panels show no data.
      cephConfig:
        global:
          mgr/prometheus/rbd_stats_pools: "ceph-blockpool"
          mgr/prometheus/rbd_stats_pools_refresh_interval: "60"
      cleanupPolicy:
        wipeDevicesFromOtherClusters: false
      mon:
        count: 3
        allowMultiplePerNode: false
      mgr:
        count: 2
        allowMultiplePerNode: false
      dashboard:
        enabled: true
        ssl: false
      crashCollector:
        disable: false
      storage:
        useAllNodes: false
        useAllDevices: false
        nodes:
        # Temporarily disabled for bare metal reprovisioning
        # - name: "rias"
        #   devicePathFilter: "/dev/disk/by-id/scsi-0QEMU_QEMU_HARDDISK_drive-scsi2"
        #   config:
        #     deviceClass: "ssd"
        #     metadataDevice: ""
        #     osdsPerDevice: "1"
        - name: "nami"
          devicePathFilter: "/dev/disk/by-id/ata-CT2000BX500SSD1_2513E9B2B5A5"
          config:
            deviceClass: "ssd"
            metadataDevice: ""
            osdsPerDevice: "1"
        - name: "marin"
          devicePathFilter: "/dev/disk/by-id/nvme-Samsung_SSD_970_EVO_Plus_1TB_S6S1NJ0TB03807K"
          config:
            deviceClass: "ssd"
            metadataDevice: ""
            osdsPerDevice: "1"
        - name: "sakura"
          devicePathFilter: "/dev/disk/by-id/nvme-Samsung_SSD_970_EVO_Plus_1TB_S6S1NS0W223961P"
          config:
            deviceClass: "ssd"
            metadataDevice: ""
            osdsPerDevice: "1"
        - name: "hanekawa"
          devicePathFilter: "/dev/disk/by-id/nvme-Samsung_SSD_970_EVO_Plus_1TB_S6S1NS0W224436E"
          config:
            deviceClass: "ssd"
            metadataDevice: ""
            osdsPerDevice: "1"

    # Storage classes configuration - using defaults
    cephBlockPools:
    - name: ceph-blockpool
      spec:
        failureDomain: host
        replicated:
          size: 3
          requireSafeReplicaSize: true
      storageClass:
        enabled: true
        name: ceph-block
        isDefault: true
        reclaimPolicy: Delete
        allowVolumeExpansion: true
        volumeBindingMode: Immediate
        parameters:
          imageFormat: "2"
          imageFeatures: layering
          csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
          csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/fstype: ext4
    cephBlockPoolsVolumeSnapshotClass:
      enabled: true
      name: csi-ceph-blockpool
      isDefault: false
      deletionPolicy: Delete

    cephFileSystems:
    - name: ceph-filesystem
      spec:
        metadataPool:
          replicated:
            size: 3
        dataPools:
        - name: data0
          failureDomain: host
          replicated:
            size: 3
        metadataServer:
          activeCount: 1
          activeStandby: true
      storageClass:
        enabled: true
        name: ceph-filesystem
        pool: data0
        reclaimPolicy: Delete
        allowVolumeExpansion: true
        volumeBindingMode: Immediate
        parameters:
          csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
          csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
          csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
          csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/fstype: ext4
    cephFileSystemVolumeSnapshotClass:
      enabled: true
      name: csi-ceph-filesystem
      isDefault: false
      deletionPolicy: Delete

    cephObjectStores:
    - name: ceph-objectstore
      spec:
        metadataPool:
          failureDomain: host
          replicated:
            size: 3
        dataPool:
          failureDomain: host
          erasureCoded:
            dataChunks: 2
            codingChunks: 1
        preservePoolsOnDelete: true
        gateway:
          sslCertificateRef:
          port: 80
          instances: 1
          priorityClassName:
      storageClass:
        enabled: true
        name: ceph-bucket
        reclaimPolicy: Delete
        volumeBindingMode: Immediate
        parameters:
          region: us-east-1
