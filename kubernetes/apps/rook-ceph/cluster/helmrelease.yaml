---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/helmrelease-helm-v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: rook-ceph-cluster
spec:
  interval: 1h
  chartRef:
    kind: OCIRepository
    name: rook-ceph-cluster
    namespace: rook-ceph
  dependsOn:
  - name: rook-ceph-operator
    namespace: rook-ceph
  install:
    remediation:
      retries: -1
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  values:
    # Rook Ceph Cluster Configuration
    # 3-node cluster with device selectors for each node
    monitoring:
      enabled: true
      createPrometheusRules: true
      prometheusRuleOverrides:
        # CRITICAL: Disable CephNodeNetworkPacketDrops alert for Intel NIC false positives
        # Intel NUCs (nami, marin) with bond0 interfaces show benign receive packet drops
        # due to hardware checksum offloading dropping invalid packets (multicast/broadcast noise)
        # Ceph cluster remains HEALTH_OK, indicating no operational impact
        # See: Intel I219/I225 NICs commonly drop packets with checksum offload enabled
        CephNodeNetworkPacketDrops:
          disabled: true
        # Disable PrometheusJobMissing alert - rook-ceph namespace excluded from VMAgent scraping
        # Metrics collection intentionally disabled to avoid overwhelming observability stack
        PrometheusJobMissing:
          disabled: true
        # Disable predict_linear disk space alert - generates false positives during image pulls
        # Replaced with threshold-based NodeDiskSpaceLow/Critical alerts in observability vmrule
        CephNodeDiskspaceWarning:
          disabled: true

    ingress:
      dashboard:
        enabled: false

    toolbox:
      enabled: true

    cephClusterSpec:
      # Recovery/backfill tuning for NVMe OSDs (Samsung 990 PRO/970 EVO Plus)
      # Aggressive settings to prevent backfill_wait queue buildup and slow ops
      # Default osd_recovery_max_active=1 causes cascading delays with client I/O
      # Reference: https://docs.ceph.com/en/latest/rados/configuration/osd-config-ref/
      cephConfig:
        global:
          # Enable mClock override to allow custom recovery tuning for NVMe OSDs
          # Required because mClock scheduler gates osd_max_backfills/osd_recovery_max_active
          # Without this, aggressive recovery settings are ignored, causing queue buildup
          osd_mclock_override_recovery_settings: "true"
          # Backfill/recovery parallelism - balance speed vs client I/O impact
          osd_max_backfills: "3"
          osd_recovery_max_active: "12"
          # Small sleep prevents recovery from monopolizing OSD threads
          osd_recovery_sleep_ssd: "0.025"
          # Lower priority vs client I/O (default=3, client ops default to 63)
          osd_recovery_op_priority: "2"
      cleanupPolicy:
        wipeDevicesFromOtherClusters: false
      mon:
        count: 3
        allowMultiplePerNode: false
      mgr:
        count: 2
        allowMultiplePerNode: false
      dashboard:
        enabled: true
        ssl: false
      crashCollector:
        disable: false
      storage:
        useAllNodes: false
        useAllDevices: false
        nodes:
        - name: "lucy"
          devicePathFilter: "/dev/disk/by-id/nvme-CT2000P3PSSD8_2510E9ACCA3D"
          config:
            deviceClass: "ssd"
            metadataDevice: ""
            osdsPerDevice: "1"
        - name: "nami"
          devicePathFilter: "/dev/disk/by-id/nvme-Samsung_SSD_990_PRO_2TB_S7L9NJ0Y903689D"
          config:
            deviceClass: "ssd"
            metadataDevice: ""
            osdsPerDevice: "1"
        - name: "marin"
          devicePathFilter: "/dev/disk/by-id/nvme-Samsung_SSD_970_EVO_Plus_1TB_S6S1NJ0TB03807K"
          config:
            deviceClass: "ssd"
            metadataDevice: ""
            osdsPerDevice: "1"
        - name: "sakura"
          devicePathFilter: "/dev/disk/by-id/nvme-Samsung_SSD_970_EVO_Plus_1TB_S6S1NS0W223961P"
          config:
            deviceClass: "ssd"
            metadataDevice: ""
            osdsPerDevice: "1"
        - name: "hanekawa"
          devicePathFilter: "/dev/disk/by-id/nvme-Samsung_SSD_970_EVO_Plus_1TB_S6S1NS0W224436E"
          config:
            deviceClass: "ssd"
            metadataDevice: ""
            osdsPerDevice: "1"
      # Resource limits - prevent memory pressure
      # MGR: peaked at 928Mi with 1Gi limit (90% alert threshold)
      # MGR-sidecar: spiked to 93Mi of 100Mi default (only exists when mgr.count: 2)
      # OSD: observed using 1.7GB during backfill operations
      resources:
        mgr:
          limits:
            memory: 2Gi
        mgr-sidecar:
          limits:
            memory: 200Mi
        osd:
          limits:
            memory: 8Gi

    # Storage classes configuration - using defaults
    cephBlockPools:
    - name: ceph-blockpool
      spec:
        failureDomain: host
        replicated:
          size: 3
          requireSafeReplicaSize: true
      storageClass:
        enabled: true
        name: ceph-block
        isDefault: true
        reclaimPolicy: Delete
        allowVolumeExpansion: true
        volumeBindingMode: Immediate
        parameters:
          imageFormat: "2"
          imageFeatures: layering
          csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
          csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/fstype: ext4
    cephBlockPoolsVolumeSnapshotClass:
      enabled: true
      name: csi-ceph-blockpool
      isDefault: false
      deletionPolicy: Delete

    cephFileSystems:
    - name: ceph-filesystem
      spec:
        metadataPool:
          replicated:
            size: 3
        dataPools:
        - name: data0
          failureDomain: host
          replicated:
            size: 3
        metadataServer:
          activeCount: 1
          activeStandby: true
      storageClass:
        enabled: true
        name: ceph-filesystem
        pool: data0
        reclaimPolicy: Delete
        allowVolumeExpansion: true
        volumeBindingMode: Immediate
        parameters:
          csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
          csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
          csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
          csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/fstype: ext4
    cephFileSystemVolumeSnapshotClass:
      enabled: true
      name: csi-ceph-filesystem
      isDefault: false
      deletionPolicy: Delete

    # RGW (S3) disabled - using external Garage S3 on nezuko instead
    cephObjectStores: []
