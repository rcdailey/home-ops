---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/helmrelease-helm-v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: rook-ceph-cluster
spec:
  interval: 1h
  chart:
    spec:
      chart: rook-ceph-cluster
      version: v1.18.3
      sourceRef:
        kind: HelmRepository
        name: rook-ceph
        namespace: rook-ceph
  dependsOn:
  - name: rook-ceph-operator
    namespace: rook-ceph
  install:
    remediation:
      retries: -1
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  values:
    # Rook Ceph Cluster Configuration
    # 3-node cluster with device selectors for each node
    monitoring:
      enabled: true
      createPrometheusRules: true
      prometheusRuleOverrides:
        # CRITICAL: Patch CephNodeDiskspaceWarning to exclude NFS/tmpfs from disk space alerts
        # Without this, NFS volumes (e.g., 160TB media share) trigger false warnings
        # See: https://github.com/rook/rook/issues/9082, https://github.com/rook/rook/pull/9837
        CephNodeDiskspaceWarning:
          expr: >-
            predict_linear(node_filesystem_free_bytes{device=~"/.*",fstype!~"nfs.*|tmpfs"}[2d], 3600 * 24 * 5)
            *on(instance) group_left(nodename) node_uname_info < 0

        # CRITICAL: Disable CephNodeNetworkPacketDrops alert
        # This alert triggers on transient packet drops during Ceph I/O bursts on nami node (192.168.1.50)
        #
        # Root cause analysis:
        # - nami: 1.82TB OSD + control plane (mon-d) = 804K RX drops (0.295%)
        # - marin: 0.91TB OSD + control plane (mon-a, mgr-a) = 439K RX drops (0.221%)
        # - sakura/hanekawa: 0.91TB OSD workers = 332 RX drops (0.000%)
        # - All nodes use same Intel e1000e NICs with identical tuning (4096 ring buffers, TSO/GSO/GRO disabled)
        #
        # Conclusion: Not a hardware/driver issue - nami's disproportionate workload (2x OSD + control plane)
        # creates network load exceeding alert thresholds (0.5% drop rate + 10 pkt/s) during traffic bursts.
        # Workers with same NICs show zero drops, proving hardware is capable.
        # Cumulative 0.29% drop rate is acceptable with no operational impact.
        # Alternative solutions (hardware upgrade, OSD rebalancing) are unnecessary for this drop rate.
        #
        # Disabling via expr: "0" prevents false positives while maintaining rule structure
        CephNodeNetworkPacketDrops:
          expr: "0"

    ingress:
      dashboard:
        enabled: false

    cephClusterSpec:
      cleanupPolicy:
        wipeDevicesFromOtherClusters: false
      mon:
        count: 3
        allowMultiplePerNode: false
      mgr:
        count: 2
        allowMultiplePerNode: false
      dashboard:
        enabled: true
        ssl: false
      crashCollector:
        disable: false
      storage:
        useAllNodes: false
        useAllDevices: false
        nodes:
        - name: "rias"
          devicePathFilter: "/dev/disk/by-id/scsi-0QEMU_QEMU_HARDDISK_drive-scsi2"
          config:
            deviceClass: "ssd"
            metadataDevice: ""
            osdsPerDevice: "1"
        - name: "nami"
          devicePathFilter: "/dev/disk/by-id/ata-CT2000BX500SSD1_2513E9B2B5A5"
          config:
            deviceClass: "ssd"
            metadataDevice: ""
            osdsPerDevice: "1"
        - name: "marin"
          devicePathFilter: "/dev/disk/by-id/nvme-Samsung_SSD_970_EVO_Plus_1TB_S6S1NJ0TB03807K"
          config:
            deviceClass: "ssd"
            metadataDevice: ""
            osdsPerDevice: "1"
        - name: "sakura"
          devicePathFilter: "/dev/disk/by-id/nvme-Samsung_SSD_970_EVO_Plus_1TB_S6S1NS0W223961P"
          config:
            deviceClass: "ssd"
            metadataDevice: ""
            osdsPerDevice: "1"
        - name: "hanekawa"
          devicePathFilter: "/dev/disk/by-id/nvme-Samsung_SSD_970_EVO_Plus_1TB_S6S1NS0W224436E"
          config:
            deviceClass: "ssd"
            metadataDevice: ""
            osdsPerDevice: "1"

    # Storage classes configuration - using defaults
    cephBlockPools:
    - name: ceph-blockpool
      spec:
        failureDomain: host
        replicated:
          size: 3
          requireSafeReplicaSize: true
      storageClass:
        enabled: true
        name: ceph-block
        isDefault: true
        reclaimPolicy: Delete
        allowVolumeExpansion: true
        volumeBindingMode: Immediate
        parameters:
          imageFormat: "2"
          imageFeatures: layering
          csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
          csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
          csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
          csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
          csi.storage.k8s.io/fstype: ext4
    cephBlockPoolsVolumeSnapshotClass:
      enabled: true
      name: csi-ceph-blockpool
      isDefault: false
      deletionPolicy: Delete

    cephFileSystems:
    - name: ceph-filesystem
      spec:
        metadataPool:
          replicated:
            size: 3
        dataPools:
        - name: data0
          failureDomain: host
          replicated:
            size: 3
        metadataServer:
          activeCount: 1
          activeStandby: true
      storageClass:
        enabled: true
        name: ceph-filesystem
        pool: data0
        reclaimPolicy: Delete
        allowVolumeExpansion: true
        volumeBindingMode: Immediate
        parameters:
          csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
          csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
          csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
          csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
          csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
          csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
          csi.storage.k8s.io/fstype: ext4
    cephFileSystemVolumeSnapshotClass:
      enabled: true
      name: csi-ceph-filesystem
      isDefault: false
      deletionPolicy: Delete

    cephObjectStores:
    - name: ceph-objectstore
      spec:
        metadataPool:
          failureDomain: host
          replicated:
            size: 3
        dataPool:
          failureDomain: host
          erasureCoded:
            dataChunks: 2
            codingChunks: 1
        preservePoolsOnDelete: true
        gateway:
          sslCertificateRef:
          port: 80
          instances: 1
          priorityClassName:
      storageClass:
        enabled: true
        name: ceph-bucket
        reclaimPolicy: Delete
        volumeBindingMode: Immediate
        parameters:
          region: us-east-1
