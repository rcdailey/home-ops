---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/helmrelease-helm-v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: rook-ceph-cluster
spec:
  interval: 1h
  chartRef:
    kind: OCIRepository
    name: rook-ceph-cluster
    namespace: rook-ceph
  dependsOn:
  - name: rook-ceph-operator
    namespace: rook-ceph
  install:
    remediation:
      retries: -1
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  values:
    # Rook Ceph Cluster Configuration
    # 3-node cluster with device selectors for each node
    monitoring:
      enabled: true
      createPrometheusRules: true
      prometheusRuleOverrides:
        # CRITICAL: Patch CephNodeDiskspaceWarning to exclude NFS/tmpfs from disk space alerts
        # Without this, NFS volumes (e.g., 160TB media share) trigger false warnings
        # See: https://github.com/rook/rook/issues/9082, https://github.com/rook/rook/pull/9837
        CephNodeDiskspaceWarning:
          expr: >-
            predict_linear(node_filesystem_free_bytes{device=~"/.*",fstype!~"nfs.*|tmpfs"}[2d], 3600 * 24 * 5)
            *on(instance) group_left(nodename) node_uname_info < 0
        # CRITICAL: Disable CephNodeNetworkPacketDrops alert for Intel NIC false positives
        # Intel NUCs (nami, marin) with eno1 interfaces show benign receive packet drops
        # due to hardware checksum offloading dropping invalid packets (multicast/broadcast noise)
        # Ceph cluster remains HEALTH_OK, indicating no operational impact
        # See: Intel I219/I225 NICs commonly drop packets with checksum offload enabled
        CephNodeNetworkPacketDrops:
          disabled: true

    ingress:
      dashboard:
        enabled: false

    toolbox:
      enabled: true

    cephClusterSpec:
      # Enable per-RBD image metrics for PVC monitoring dashboards
      # Required for ceph_rbd_* metrics (read/write IOPS, throughput, latency)
      # to populate Grafana dashboard panels. Without this, only cluster-level
      # metrics are available and per-PVC RBD performance panels show no data.
      #
      # Recovery/backfill tuning for NVMe OSDs (Samsung 990 PRO/970 EVO Plus)
      # Aggressive settings to prevent backfill_wait queue buildup and slow ops
      # Default osd_recovery_max_active=1 causes cascading delays with client I/O
      # Reference: https://docs.ceph.com/en/latest/rados/configuration/osd-config-ref/
      cephConfig:
        global:
          mgr/prometheus/rbd_stats_pools: "ceph-blockpool"
          mgr/prometheus/rbd_stats_pools_refresh_interval: "60"
          # Enable mClock override to allow custom recovery tuning for NVMe OSDs
          # Required because mClock scheduler gates osd_max_backfills/osd_recovery_max_active
          # Without this, aggressive recovery settings are ignored, causing queue buildup
          osd_mclock_override_recovery_settings: "true"
          # Backfill/recovery parallelism - balance speed vs client I/O impact
          osd_max_backfills: "3"
          osd_recovery_max_active: "12"
          # Small sleep prevents recovery from monopolizing OSD threads
          osd_recovery_sleep_ssd: "0.025"
          # Lower priority vs client I/O (default=3, client ops default to 63)
          osd_recovery_op_priority: "2"
      cleanupPolicy:
        wipeDevicesFromOtherClusters: false
      mon:
        count: 3
        allowMultiplePerNode: false
      mgr:
        count: 2
        allowMultiplePerNode: false
      dashboard:
        enabled: true
        ssl: false
      crashCollector:
        disable: false
      storage:
        useAllNodes: false
        useAllDevices: false
        nodes:
        - name: "lucy"
          devicePathFilter: "/dev/disk/by-id/nvme-CT2000P3PSSD8_2510E9ACCA3D"
          config:
            deviceClass: "ssd"
            metadataDevice: ""
            osdsPerDevice: "1"
        - name: "nami"
          devicePathFilter: "/dev/disk/by-id/nvme-Samsung_SSD_990_PRO_2TB_S7L9NJ0Y903689D"
          config:
            deviceClass: "ssd"
            metadataDevice: ""
            osdsPerDevice: "1"
        - name: "marin"
          devicePathFilter: "/dev/disk/by-id/nvme-Samsung_SSD_970_EVO_Plus_1TB_S6S1NJ0TB03807K"
          config:
            deviceClass: "ssd"
            metadataDevice: ""
            osdsPerDevice: "1"
        - name: "sakura"
          devicePathFilter: "/dev/disk/by-id/nvme-Samsung_SSD_970_EVO_Plus_1TB_S6S1NS0W223961P"
          config:
            deviceClass: "ssd"
            metadataDevice: ""
            osdsPerDevice: "1"
        - name: "hanekawa"
          devicePathFilter: "/dev/disk/by-id/nvme-Samsung_SSD_970_EVO_Plus_1TB_S6S1NS0W224436E"
          config:
            deviceClass: "ssd"
            metadataDevice: ""
            osdsPerDevice: "1"
      # OSD resource limits - prevent memory pressure during recovery
      # OSD.2 observed using 1.7GB during backfill operations
      resources:
        osd:
          limits:
            cpu: "2"
            memory: "4Gi"
          requests:
            cpu: "500m"
            memory: "1Gi"

    # Storage classes configuration - using defaults
    cephBlockPools:
    - name: ceph-blockpool
      spec:
        failureDomain: host
        replicated:
          size: 3
          requireSafeReplicaSize: true
      storageClass:
        enabled: true
        name: ceph-block
        isDefault: true
        reclaimPolicy: Delete
        allowVolumeExpansion: true
        volumeBindingMode: Immediate
        parameters:
          imageFormat: "2"
          imageFeatures: layering
          csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
          csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/fstype: ext4
    cephBlockPoolsVolumeSnapshotClass:
      enabled: true
      name: csi-ceph-blockpool
      isDefault: false
      deletionPolicy: Delete

    cephFileSystems:
    - name: ceph-filesystem
      spec:
        metadataPool:
          replicated:
            size: 3
        dataPools:
        - name: data0
          failureDomain: host
          replicated:
            size: 3
        metadataServer:
          activeCount: 1
          activeStandby: true
      storageClass:
        enabled: true
        name: ceph-filesystem
        pool: data0
        reclaimPolicy: Delete
        allowVolumeExpansion: true
        volumeBindingMode: Immediate
        parameters:
          csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
          csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
          csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
          csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/fstype: ext4
    cephFileSystemVolumeSnapshotClass:
      enabled: true
      name: csi-ceph-filesystem
      isDefault: false
      deletionPolicy: Delete

    # RGW (S3) disabled - using external Garage S3 on nezuko instead
    cephObjectStores: []
