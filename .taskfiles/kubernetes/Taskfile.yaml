---
# yaml-language-server: $schema=https://taskfile.dev/schema.json
version: '3'

silent: true

tasks:

  list-nodes:
    desc: List Kubernetes nodes with status
    cmd: kubectl get nodes -o wide

  # Non-graceful node shutdown handling
  # See: https://github.com/rook/rook/issues/1507
  # See: https://github.com/ceph/ceph-csi/blob/devel/docs/design/proposals/non-graceful-node-shutdown.md

  fence-node:
    desc: Add out-of-service taint to a dead node (enables CSI fencing)
    vars:
      NODE:
        sh: |
          if [ -z "{{.NODE}}" ]; then
            kubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}{end}' | fzf --prompt="Select node to fence: "
          else
            echo "{{.NODE}}"
          fi
    cmd: kubectl taint node {{.NODE}} node.kubernetes.io/out-of-service=nodeshutdown:NoExecute --overwrite
    preconditions:
      - which kubectl fzf

  unfence-node:
    desc: Remove out-of-service taint from a node (ONLY after full power cycle!)
    vars:
      NODE:
        sh: |
          if [ -z "{{.NODE}}" ]; then
            kubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}{end}' | fzf --prompt="Select node to unfence: "
          else
            echo "{{.NODE}}"
          fi
    prompt: |
      WARNING: Only remove this taint AFTER the node has completed a full power cycle!

      If the node still has stale mounts from before the crash, removing this taint
      can cause data corruption. The node must be fully powered off and back on to
      clear any cached writes or stale mount state.

      Are you sure {{.NODE}} has completed a full power cycle?
    cmd: kubectl taint node {{.NODE}} node.kubernetes.io/out-of-service=nodeshutdown:NoExecute-
    preconditions:
      - which kubectl fzf

  show-fenced-nodes:
    desc: Show nodes with out-of-service taint
    cmd: kubectl get nodes -o jsonpath='{range .items[?(@.spec.taints)]}{.metadata.name}{"\t"}{range .spec.taints[*]}{.key}={.value}:{.effect}{" "}{end}{"\n"}{end}' | rg out-of-service || echo "No fenced nodes"

  # CSI fencing configuration

  enable-fencing:
    desc: Enable fencing on ceph-csi RBD driver (patches Driver CRD)
    cmd: kubectl patch drivers.csi.ceph.io -n rook-ceph rook-ceph.rbd.csi.ceph.com --type merge -p '{"spec":{"enableFencing":true}}'
    preconditions:
      - kubectl get drivers.csi.ceph.io -n rook-ceph rook-ceph.rbd.csi.ceph.com

  disable-fencing:
    desc: Disable fencing on ceph-csi RBD driver
    cmd: kubectl patch drivers.csi.ceph.io -n rook-ceph rook-ceph.rbd.csi.ceph.com --type merge -p '{"spec":{"enableFencing":false}}'
    preconditions:
      - kubectl get drivers.csi.ceph.io -n rook-ceph rook-ceph.rbd.csi.ceph.com

  show-fencing-status:
    desc: Show current fencing configuration
    cmd: kubectl get drivers.csi.ceph.io -n rook-ceph -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}enableFencing={.spec.enableFencing}{"\n"}{end}'

  # Node utilities

  node-shell:
    desc: Open a debug shell on a node
    vars:
      NODE:
        sh: |
          if [ -z "{{.NODE}}" ]; then
            kubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}{end}' | fzf --prompt="Select node: "
          else
            echo "{{.NODE}}"
          fi
    cmd: kubectl debug node/{{.NODE}} -n default -it --image=alpine:latest --profile sysadmin
    preconditions:
      - which kubectl fzf

  prune-pods:
    desc: Delete pods in Failed, Pending, or Succeeded state
    cmd: |
      for phase in Failed Pending Succeeded; do
        kubectl delete pods -A --field-selector status.phase="$phase" --ignore-not-found=true
      done

  force-delete-terminating:
    desc: Force delete pods stuck in Terminating state on a specific node
    vars:
      NODE:
        sh: |
          if [ -z "{{.NODE}}" ]; then
            kubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}{end}' | fzf --prompt="Select node: "
          else
            echo "{{.NODE}}"
          fi
    prompt: This will force delete all Terminating pods on {{.NODE}}. Continue?
    cmd: |
      kubectl get pods -A --field-selector spec.nodeName={{.NODE}} -o json | \
        jq -r '.items[] | select(.metadata.deletionTimestamp != null) | "\(.metadata.namespace) \(.metadata.name)"' | \
        xargs -r -n2 sh -c 'kubectl delete pod -n $0 $1 --force --grace-period=0'
    preconditions:
      - which kubectl jq fzf

  delete-stale-attachments:
    desc: Delete stale VolumeAttachments for a dead node
    vars:
      NODE:
        sh: |
          if [ -z "{{.NODE}}" ]; then
            kubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}{end}' | fzf --prompt="Select node: "
          else
            echo "{{.NODE}}"
          fi
    prompt: This will delete all VolumeAttachments for {{.NODE}}. Continue?
    cmd: |
      kubectl get volumeattachments -o json | \
        jq -r '.items[] | select(.spec.nodeName=="{{.NODE}}") | .metadata.name' | \
        xargs -r -I {} kubectl delete volumeattachment {} --force --grace-period=0
    preconditions:
      - which kubectl jq fzf
